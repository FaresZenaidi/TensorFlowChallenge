{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 Align=center> Challenge Machine Learning Avancée - MDI341  </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the Challenge\n",
    "Lean a predictive system which predicts templates (vector of 128 elements) from low resolution images, in such a way to minize the gap to original templates that were predicted form high resolution images.\n",
    "\n",
    "### Datasets and general procedure to generate the predictions\n",
    "* Fit the model on the training set (X_train, Y_train) of dimension (100'000 x 2304, 100'000 x 128)\n",
    "* Evaluate the performance on the validation set (X_valid, Y_valid) of dimension (10'000 x 2304 , 10'000 x 128)\n",
    "* Prediction on the testing set X_test of dimension 10'000 x 2304 and generate Y_test of dimension 10'000 x 128\n",
    "\n",
    "### Pre-processing of the data and data augmentation\n",
    "* Centering-Reduction of the images so that they have zero mean and unit variance. \n",
    "* By rotating, mirroring, adjusting contrast, etc. it is possible to generate additional images from the original ones. Thanks to this data augmentation thechnique, it is possible to extend our training set with slightly modified images with known labels, which makes our model generalize better\n",
    "\n",
    "### Coding of the model\n",
    "Since we are manipulating images in this challenge, I decided to implement a Convolutional Neural Network model. To do so, I have implemented it using TensorFlow initially and afterwards TFLearn and Keras (which are Deep Learning Libraries which run on top of TensorFlow).\n",
    "\n",
    "### Architecture of the Convolutional Neural Networks\n",
    "We were asked to build a CNN architecture that contains less than 50k parameters. Therefore, I implemented an architecture that respects this constraint and that has the following form : \n",
    "* Convolutional Layer 1 : 4 x 4 kernel with 10 outputs (filtered images). A relu activation is applied on these outputs. \n",
    "* Pooling 1 : (2, 2) window size with a stride of 2, so that the size of the filtered images is reduced by 2.\n",
    "* Convolutional Layer 2 : 4 x 4 kernel with 10 outputs. A relu activation is applied on these outputs. \n",
    "* Pooling 2 : (2, 2) window size with a stride of 2, so that the size of the filtered images is reduced by 2.\n",
    "* Convolutional Layer 3 : 4 x 4 kernel with 10 outputs. A relu activation is applied on these outputs. \n",
    "* Pooling 3 : (2, 2) window size with a stride of 2, so that the size of the filtered images is reduced by 2.\n",
    "* Fully Connected Layer 3 : which takes as input the output of Pooling 3 (6x6x10) and flattening it on a single vector and connected it to an output layer of dimension 128. \n",
    "\n",
    "This architecture has 49'628 parameters.\n",
    "\n",
    "To avoid overfitting, I have added a dropout of 0.75 on the Fully Connected layer which allows to randomly disable, at each operation of optimization of the weights (backpropagation), a portion of connections between the nodes. \n",
    "\n",
    "### Weights initialization\n",
    "To optimize the convergence speed of the Gradient Descent procedure, I used the Xavier Initiliazer to set the initial\n",
    "weights properly (close to the global minimum).\n",
    "\n",
    "### Optimizer with an exponential decay learning rate \n",
    "For the Gradient Descent optimization part, I chose to use an AdamOptimizer which allows to minize the loss function by updating the weight matrices W and b of each layer (backpropagation operation). When training a model, it is often recommended to lower the learning rate as the training progresses. Thus, I applied an exponential decay function to a provided initial learning. \n",
    "\n",
    "For the training procedure, I process the data by batches of 128 images and I save the model every 10 batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import keras as kr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.layers.core import input_data\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "images_train_fname    = 'data_train.bin'\n",
    "templates_train_fname = 'fv_train.bin'\n",
    "\n",
    "# Validation set\n",
    "images_valid_fname    = 'data_valid.bin'\n",
    "templates_valid_fname = 'fv_valid.bin'\n",
    "\n",
    "# Testing set\n",
    "images_test_fname     = 'data_test.bin'\n",
    "\n",
    "# number of images\n",
    "num_train_images = 100000\n",
    "num_valid_images = 10000\n",
    "num_test_images  = 10000\n",
    "\n",
    "# size of the images 48*48 pixels in gray levels\n",
    "image_dim = 48 * 48\n",
    "\n",
    "# dimension of the templates\n",
    "template_dim = 128\n",
    "\n",
    "# read the training files\n",
    "with open(images_train_fname, 'rb') as f:\n",
    "    X_train = np.fromfile(f, dtype=np.uint8, count=num_train_images * image_dim).astype(np.float32)\n",
    "    X_train = X_train.reshape(num_train_images, image_dim)\n",
    "\n",
    "with open(templates_train_fname, 'rb') as f:\n",
    "    Y_train = np.fromfile(f, dtype=np.float32, count=num_train_images * template_dim)\n",
    "    Y_train = Y_train.reshape(num_train_images, template_dim)\n",
    "\n",
    "# read the validation files\n",
    "with open(images_valid_fname, 'rb') as f:\n",
    "    X_valid = np.fromfile(f, dtype=np.uint8, count=num_valid_images * image_dim).astype(np.float32)\n",
    "    X_valid = X_valid.reshape(num_valid_images, image_dim)\n",
    "\n",
    "with open(templates_valid_fname, 'rb') as f:\n",
    "    Y_valid = np.fromfile(f, dtype=np.float32, count=num_valid_images * template_dim)\n",
    "    Y_valid = Y_valid.reshape(num_valid_images, template_dim)\n",
    "\n",
    "# read the test file\n",
    "with open(images_test_fname, 'rb') as f:\n",
    "    X_test = np.fromfile(f, dtype=np.uint8, count=num_test_images * image_dim).astype(np.float32)\n",
    "    X_test = X_test.reshape(num_test_images, image_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Investigation of the shape of the data\n",
    "print(\"Shape X_train : {}\".format(X_train.shape))\n",
    "print(\"Shape Y_train : {}\\n\".format(Y_train.shape))\n",
    "print(\"Shape X_valid : {}\".format(X_valid.shape))\n",
    "print(\"Shape Y_valid: {}\\n\".format(Y_valid.shape))\n",
    "print(\"Shape X_test : {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(48, 48), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Centering-Reduction and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Real-time data preprocessing\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_rotation(max_angle=25.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measure: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pred_score(y_true, y_pred):\n",
    "    err_y = np.mean((y_true - y_pred) ** 2)\n",
    "    return err_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Simple linear regression \n",
    "\n",
    "# Learn the matrix W from the training data\n",
    "W = np.linalg.solve(np.dot(X_train.T, X_train),\n",
    "                    np.dot(X_train.T, Y_train))\n",
    "\n",
    "# Training prediction\n",
    "Y_train_pred = np.dot(X_train, W)\n",
    "\n",
    "# Training error\n",
    "err_train = compute_pred_score(Y_train, Y_train_pred)\n",
    "\n",
    "print('Error on the training data : %s' % err_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monitor the validation error\n",
    "Y_valid_pred = np.dot(X_valid, W)\n",
    "err_valid = compute_pred_score(Y_valid, Y_valid_pred)\n",
    "\n",
    "print('Error on the validation data : %s' % err_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the prediction\n",
    "test_template_pred = np.dot(X_test, W)\n",
    "\n",
    "f = open('template_pred.bin', 'wb')\n",
    "for i in range(num_test_images):\n",
    "    f.write(test_template_pred[i, :])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayel Convolutional Network (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution(x, W, b):  \n",
    "    # convolution of input image x by weight tensor W => generation of output features (filtered images).\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def max_pooling(x, k=2):  \n",
    "    # by default (window size : 2 x 2) with a stride of 2.\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):  \n",
    "    # Weight W initialization\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):  \n",
    "    # Bias b initialization\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    \n",
    "    # 1st Convolutional Layer\n",
    "    with tf.variable_scope('conv1'):\n",
    "        x = tf.reshape(x, shape=[-1, 48, 48, 1])\n",
    "        prep_x = input_data(placeholder=x, data_preprocessing=img_prep, data_augmentation=img_aug)\n",
    "        h_conv1 = convolution(prep_x, weights['wc1'], biases['bc1'])\n",
    "    with tf.variable_scope('pool1'):\n",
    "        h_pool1 = max_pooling(h_conv1)\n",
    "    \n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "    with tf.variable_scope('conv2'):\n",
    "        h_conv2 = convolution(h_pool1, weights['wc2'],  biases['bc2'])\n",
    "    with tf.variable_scope('pool2'):\n",
    "        h_pool2 = max_pooling(h_conv2)\n",
    "    \n",
    "    \n",
    "    # 3rd Convolutional Layer\n",
    "    with tf.variable_scope('conv3'):\n",
    "        h_conv3 = convolution(h_pool2, weights['wc3'], biases['bc3'])\n",
    "    with tf.variable_scope('pool3'):\n",
    "        h_pool3 = max_pooling(h_conv3)\n",
    "\n",
    "        \n",
    "    # Fully (Densely) Connected Layer \n",
    "    with tf.variable_scope('fc'):\n",
    "        h_pool3_flat = tf.reshape(h_pool3, [-1, weights['wf1'].get_shape().as_list()[0]])\n",
    "        h_fc1 = tf.matmul(h_pool3_flat, weights['wf1']) + biases['bf1']\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, dropout)  # Apply Dropout\n",
    "    \n",
    "    return h_fc1_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class images_container():\n",
    "\n",
    "    def __init__(self, _X, _y, _batch_size):\n",
    "        self.current_position = 0\n",
    "        self.batch_size = _batch_size\n",
    "        \n",
    "        self.X = _X\n",
    "        self.y = _y\n",
    "        \n",
    "    def next_batch(self):\n",
    "        if(self.current_position + self.batch_size > len(self.X)):\n",
    "            self.current_position = 0\n",
    "        \n",
    "        X_batch = self.X[self.current_position: self.current_position + self.batch_size]\n",
    "        y_batch = self.y[self.current_position: self.current_position + self.batch_size]\n",
    "            \n",
    "        self.current_position += self.batch_size\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get total number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_param():\n",
    "    \"\"\"\n",
    "    Be sure you are not over the 50 K !!!\n",
    "    \"\"\"\n",
    "    total_parameters = 0\n",
    "    #iterating over all variables\n",
    "    for variable in tf.trainable_variables():  \n",
    "        local_parameters=1\n",
    "        shape = variable.get_shape()  #getting shape of a variable\n",
    "        for i in shape:\n",
    "            local_parameters*=i.value  #mutiplying dimension values\n",
    "        total_parameters+=local_parameters\n",
    "    \n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph inputs\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, image_dim], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, template_dim], name=\"Y_placeholder\")\n",
    "dropout = tf.placeholder(tf.float32, name=\"dropout\") \n",
    "\n",
    "tf.summary.scalar('dropout_keep_probability', dropout)\n",
    "\n",
    "# Weights and biases\n",
    "weights = {\n",
    "    # 1st conv. layer : 4x4 filter taking 1 input (image) and generating 10 outputs\n",
    "    'wc1': weight_variable([4, 4, 1, 10]),\n",
    "    # Pooling\n",
    "    # 2nd conv. layer : 4x4 filter, 10 inputs, 10 outputs\n",
    "    'wc2': weight_variable([4, 4, 10, 10]),\n",
    "    # 3rd conv. layer : 4x4 filter, 10 inputs, 10 outputs\n",
    "    'wc3': weight_variable([4, 4, 10, 10]),\n",
    "    # fully connected, 6*6*10 inputs, 128 outputs\n",
    "    'wf1': weight_variable([6 * 6 * 10, 128]),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': bias_variable([10]),\n",
    "    'bc2': bias_variable([10]),\n",
    "    'bc3': bias_variable([10]),\n",
    "    'bf1': bias_variable([128]),\n",
    "}\n",
    "\n",
    "# Building the model\n",
    "pred = conv_net(X, weights, biases, dropout)\n",
    "\n",
    "# Get total number of parameters\n",
    "print(\"Total number of parameters : {}\".format(get_total_param()))\n",
    "\n",
    "# Loss function\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_sum(tf.square(Y - pred))\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Merge all summaries \n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Optimizer\n",
    "start_learning_rate = 0.01\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.95\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "# Decaying learning rate\n",
    "learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, decay_steps, decay_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Simulation parameters\n",
    "BATCH_SIZE = 128\n",
    "DISPLAY_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "n_batches = int(num_train_images / BATCH_SIZE)  # to process all the training dataset (781 batches in total)\n",
    "N_EPOCHS = 5  # train the model N_EPOCHS times\n",
    "\n",
    "# Train the model, write training summaries by adding them to the summary_writer \n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())  # Run session and initializing variables\n",
    "    saver = tf.train.Saver() # To save the model \n",
    "    writer = tf.summary.FileWriter(\"./log_files\", sess.graph)  # Generate tons of logs (including summaries)\n",
    "\n",
    "    train_data = images_container(X_train, Y_train, BATCH_SIZE)\n",
    "    step = 1\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step < (N_EPOCHS * n_batches):  # 3905 iterations\n",
    "        batch_X, batch_Y = train_data.next_batch()\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={X: batch_X, Y: batch_Y, dropout: DROPOUT})\n",
    "\n",
    "        if step % DISPLAY_STEP == 0:\n",
    "            loss_train, summary = sess.run([loss, merged], feed_dict={X: batch_X, Y: batch_Y, dropout: 1.0}) \n",
    "            writer.add_summary(summary, step)\n",
    "            \n",
    "            saver.save(sess, \"./log_files/model.ckpt\", step)\n",
    "            \n",
    "            # Make predictions and compute score on validation set\n",
    "            Y_valid_pred = sess.run(pred, {X: X_valid, dropout: 1.0})\n",
    "            # Look at the score on validation set\n",
    "            loss_valid = compute_pred_score(Y_valid_pred, Y_valid)\n",
    "            print(\"Epoch ({}/{}) - Batch ({}/{}) ==> Training loss: {}, Validation loss: {}\".format(int(step / n_batches) + 1, \n",
    "                                                    N_EPOCHS, np.mod(step, n_batches), n_batches, loss_train, loss_valid))\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    print(\"Optimization finished\")\n",
    "\n",
    "    # Predictions on the testing set\n",
    "    Y_test = sess.run(pred, {X: X_test, dropout: 1.0})\n",
    "\n",
    "    # Write prediction in test file\n",
    "    f = open('template_pred.bin', 'wb')\n",
    "    for i in range(num_test_images):\n",
    "        f.write(Y_test[i, :])\n",
    "    f.close()\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayel Convolutional Network (TFLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn import Momentum\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.metrics import R2\n",
    "from tflearn.losses import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1, 48, 48, 1])\n",
    "X_valid = X_valid.reshape([-1, 48, 48, 1])\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "# Convolutional network building\n",
    "network = input_data(shape=[None, 48, 48, 1],\n",
    "                     data_preprocessing=img_prep,\n",
    "                     data_augmentation=img_aug)\n",
    "\n",
    "trunc_norm = tflearn.initializations.truncated_normal(stddev=0.1)\n",
    "\n",
    "network = conv_2d(network, 10, 4, activation='prelu', weights_init='xavier', name='conv1')\n",
    "network = max_pool_2d(network, 2, name='pool1')\n",
    "\n",
    "network = conv_2d(network, 10, 4, activation='prelu', weights_init='xavier', name='conv2')\n",
    "network = max_pool_2d(network, 2, name='pool2')\n",
    "\n",
    "network = conv_2d(network, 10, 4, activation='prelu', weights_init='xavier', name='conv3')\n",
    "network = max_pool_2d(network, 2, name='pool3')\n",
    "\n",
    "network = fully_connected(network, 128, activation='prelu', weights_init='xavier', name ='fc1', regularizer='L2')\n",
    "network = dropout(network, 0.75)\n",
    "\n",
    "network = regression(network, optimizer='adam', loss='mean_square', learning_rate=0.0001)\n",
    "\n",
    "\n",
    "# Train using classifier\n",
    "model = tflearn.DNN(network, tensorboard_verbose=3, checkpoint_path=\"models_tflearn/model.tfl.ckpt\")\n",
    "\n",
    "model.fit(X_train, Y_train, n_epoch=40, validation_set=(X_valid, Y_valid), snapshot_step=250, run_id='cnn_image')\n",
    "\n",
    "# Save model when training is complete to a file\n",
    "model.save('model.tfl.ckpt')\n",
    "\n",
    "# Predictions on the testing set\n",
    "Y_test = model.predict(X_test)\n",
    "\n",
    "# Write prediction in test file\n",
    "f = open('template_pred.bin', 'wb')\n",
    "for i in range(num_test_images):\n",
    "    f.write(Y_test[i, :])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayel Convolutional Network (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Activation, LocallyConnected2D, AveragePooling2D, Dropout, MaxPooling2D\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(10, kernel_size=(4, 4), input_shape=(48, 48, 1), padding='same', kernel_initializer='glorot_normal', name='Conv1'))\n",
    "model.add(PReLU(shared_axes=[1,2]))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), name='pool1'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(10, kernel_size=(4, 4), padding='same', kernel_initializer='glorot_normal', name='Conv2'))\n",
    "model.add(PReLU(shared_axes=[1,2]))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), name='pool2'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(10, kernel_size=(4, 4), padding='same', kernel_initializer='glorot_normal', name='Conv3'))\n",
    "model.add(PReLU(shared_axes=[1,2]))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), name='pool3'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LocallyConnected2D(filters=128, kernel_size=(6,6), strides=(1,1)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_keras_pred_score(y_true, y_pred):\n",
    "    err_y = keras.backend.mean((y_true - y_pred) ** 2)*100.\n",
    "    return err_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "adam = Adam(lr=0.001, decay=0.00)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 35\n",
    "\n",
    "X_train = X_train.reshape([-1, 48, 48, 1])\n",
    "X_valid = X_valid.reshape([-1, 48, 48, 1])\n",
    "X_test = X_test.reshape([-1, 48, 48, 1])\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=\"./logs/weights.hdf5\", verbose=1, save_best_only=False, mode='auto', period=1)\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1,write_graph=True, write_images=True)\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=1, callbacks=[tbCallBack, checkpointer])\n",
    "\n",
    "\n",
    "model.save('model_keras.h5')  \n",
    "model = load_model('model_keras.h5')\n",
    "\n",
    "# Prediction\n",
    "Y_test = model.predict(X_test)\n",
    "f = open('model_keras.bin', 'wb')\n",
    "for i in range(num_test_images):\n",
    "    f.write(Y_test[i, :])\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
